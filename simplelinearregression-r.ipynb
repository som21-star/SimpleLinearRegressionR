{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = read.delim(\"https://raw.githubusercontent.com/AdiPersonalWorks/Random/master/student_scores%20-%20student_scores.csv\",stringsAsFactors=F, header = T, sep=\",\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"head(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(Hmisc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"describe(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list rows of data that have missing values\ndata[!complete.cases(data),]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boxplot(data$Hours)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(\"dplyr\")\nlibrary(\"ggpubr\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Density plot and Q-Q plot can be used to check normality visually\nlibrary(\"ggpubr\")\nggdensity(data$Hours, \n          main = \"Density plot of No. of study hours\",\n          xlab = \"Study Hours\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(\"car\")\nqqPlot(data$Hours)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The R function shapiro.test() can be used to perform the Shapiro-Wilk test of normality for one variable (univariate)\nshapiro.test(data$Hours)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist(data$Hours)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm = lm(Scores~Hours, data = data) #Create the linear regression\nsummary(lm) #Review the results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(data, pch = 16, col = \"blue\") #Plot the results\nabline(lm) #Add a regression line","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(lm$residuals, pch = 16, col = \"red\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(Scores~Hours, data = data)\nabline(lm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(openintro)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(e1071)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewness(data$Hours)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm_log.model = lm(log1p(Scores) ~ log1p(Hours), data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(lm_log.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data$log_hours = log(data$Hours)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist(data$log_hours)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ggdensity(data$log_hours, \n          main = \"Density plot of log transformed hours\",\n          xlab = \"Study Hours\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm_log.model2 = lm(log1p(Scores) ~ log_hours, data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(lm_log.model2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we can drop the newly log transformed column as it has no signifanct effect\ndata <- subset(data,,-c(3))\nhead(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create data partition using 80-20 rule\nlibrary(caret)\nset.seed(99)\nindex<-createDataPartition(data$Scores, p=0.60, list=FALSE)\ntrain<-data[index,]\ntest<-data[-index,]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying linear regression using lm()\nset.seed(99)\ncaret_lm<-lm(Scores~.,train)#nzv,range,YeoJohnson\ncaret_lm_tst_pred<- predict(caret_lm, test)\nplot(caret_lm_tst_pred, test$Scores,\n    xlab= \"Predicted\", ylab= \"Actual\",\n    main=\"Predicted vs Actual: Linear\", col=\"blue\", pch=18)\ngrid()\nabline(0, 1, col = \"red\", lwd = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(caret_lm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"postResample(caret_lm_tst_pred, test$Scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using caret's train() function\nset.seed(999)\ncontrol<-trainControl(method=\"repeatedcv\", number=10, repeats=3)\ncaret_lm2<-train(Scores~.,train, method=\"lm\", trControl=control, preprocess=c(\"center\",\"scale\",\"BoxCox\"))#nzv,range,YeoJohnson\nclv_lm_tst_pred2<- predict(caret_lm2, test)\nplot(clv_lm_tst_pred2, test$Scores,\n    xlab= \"Predicted\", ylab= \"Actual\",\n    main=\"Predicted vs Actual: Linear\", col=\"blue\", pch=18)\ngrid()\nabline(0, 1, col = \"red\", lwd = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(caret_lm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"postResample(clv_lm_tst_pred2, test$Scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Implementing Random Forest where I set mtry=no.of predictors/3\nlibrary(randomForest)\nset.seed(999)\nrf<-randomForest(Scores~.,train,mtry=0.33,importance=TRUE,ntrees=100)\nrf\nrf_tst_pred<- predict(rf, test)\nplot(rf_tst_pred, test$Scores,\n    xlab= \"Predicted\", ylab= \"Actual\",\n    main=\"Predicted vs Actual: Linear\", col=\"blue\", pch=18)\ngrid()\nabline(0, 1, col = \"red\", lwd = 2)\n\nsummary(rf)\n#(lm_tst_rmse2 = calc_rmse(clv_lm_tst_pred2, test$CLV))#14.4812044544613\n#summary(clv_lm2)$r.squared\npostResample(rf_tst_pred, test$Scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(lm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(lm_log.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Finally, we can conclude simple linear regression gives me the better result than Random Forest. And, Residual standard error significantly dropped in case of log transformed model output.* \n**Scores = a + Hours*b**\nYou can see the values of the intercept and the slope for the scores. These “a” and “b” values plot a line between all the points of the data. So in this case, if there is a student, who studies for 5 hours per day, intercept(a) is 2.4837 and slope(b) is 9.7758, the model predicts (on average) that student socres around 2.4837 + (5 * 9.7758) = 51.33627","metadata":{}}]}